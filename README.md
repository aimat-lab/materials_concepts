# Dataset Creation

## Create data/ folder

Create `data/` top-level folder.

## Data Fetching

Fetch all related sources: (`--folder data/` can be omitted)

`$ python downloader/download_sources.py 'materials science' --folder data/`

> This will create a `data/materials-science.sources.csv` file with all the sources.

Fetch works from single source:

`$ python downloader/download_works.py S82336448 --folder data/`

> This will create a `data/S82336448.csv` file with all the works belonging to that source.

Fetch works from all sources:

`$ python downloader/download_works.py data/materials-science.sources.csv --folder data/`

> During fetching, this will create a `{source}.csv` file for each source in `{folder}/` listing all the works which belong to that source. After downloading, these are merged automatically into a single file `materials-science.works.csv` in the specified folder.
> If the download gets interrupted, the downloaded files serve as a cache. Re-run the script, it will automatically skip sources for which the data was already fetched.

## Data Filtering

Filter the data to improve its quality:

`$ python filtering/filter_works.py materials-science.works.csv --folder data/`

> This will output a file `materials-science.filtered.works.csv` in the specified folder containing all works which sufficed the conditions.

## Data Preparation

### Cleaning abstracts

Clean the abstracts:

`$ python preparation/clean_abstracts.py materials-science.filtered.works.csv --folder data/`

> This will output a file `materials-science.cleaned.works.csv` in the specified folder containing all works with cleaned abstracts.

### Extraction

> Note: As these operations are very time consuming, they are split across several scripts and parallelized. The scripts should be run in the following order:

1. Extract 'chemical elements' from abstracts:

`$ python preparation/extract_elements.py materials-science.cleaned.works.csv --folder data/`

> This will output a file `materials-science.elements.works.csv` in the specified folder containing all works with extracted chemical elements in a separate columns `elements`.

2. Extract 'concepts' from abstracts:

`$ python preparation/extract_concepts.py materials-science.elements.works.csv {method} {colname} --folder data/`

e.g.:

`$ python preparation/extract_concepts.py materials-science.elements.works.csv rake rake_concepts --folder data/`

> This will output a file `materials-science.rake.works.csv` in the specified folder containing all works with extracted concepts according to `rake` (`{method}`) in a separate columns `rake_concepts` (`{colname}`).

**Note:** The concepts currently used are generated by utilizing a LLM (LLaMa-13B). They are stored in `data/concepts.csv` and can be joined on `ID` column.

# Classification

## Build Graph

Build concepts graph by executing the following command:

`python graph/build.py data/materials-science.llama.works.csv llama_concepts`

> Note: If you want use rake concepts, you have to first extract the rake concepts and then replace `llama_concepts` with `rake_concepts` in the command above.

> Note: The concepts are run against a filter mechanism to remove concepts which are not relevant for the domain. The filters are stored in the same file and can be extended or modified as needed.

## Generate Raw Classification Task Data

Generate training and test data for classification task: Given {n} vertex pairs, decide whether they will be connected or not in {delta} years.

```
python model/create_data.py \
 --graph_path=graph/edges.pkl \
 --data_path=model/data.pkl \
 --year_start_train=2016 \
 --year_start_test=2019 \
 --year_delta=3 \
 --edges_used_train=4_000_000 \
 --edges_used_test=1_000_000 \
 --min_links=1 \
 --max_v_degree=None \
 --verbose=True
```

## How to add new model

## Generate Embeddings

# TODO General

## Process

- [x] Create README with instructions to recreate progress
- [x] Implement cursor fetching
- [x] Work cleaning: Filter out works (no title, no abstract, retracted, paratext, english lang)
- [x] Abstract cleaning: Clean chemical elements
- [ ] Invert filenames: ...cleaned.works.csv -> ...works.cleaned.csv
- [x] Add title to abstract (before cleaning)
- [ ] Generate cleaned 'list' of all concepts
  - [x] Rake
  - [x] LLM
    - [x] Get access to BWUniCluster
    - [x] Annotate 100 abstracts with concepts
    - [x] Fine-tune LLM
    - [x] Inference: extract concepts from all abstracts
- [x] Build graph with histogram edges
  - [x] Start with reduced concept list (keep concepts n>1) and discard materials where an element appears twice
- [x] Implement Baseline
- [ ] Implement evaluation (on test set)
- [ ] Implement validation (on future data)
- [ ] Implement top performing model from kaggle challange
- [ ] Store model and graph
- [ ] Build API to query prediction service
- [ ] Build Tiny Frontend. Input: Concept (with Suggestions) -> Output: future synergies (ranked, k=10)

## Optimization

- [ ] Where to store the data?
- [ ] Data storing for works: How to store concepts (fetched and generated)
- [x] How to speed up text processing in pandas? Pandas 2.0 (x) or other option to achieve pyarrow backend
- [ ] Dockerize what comes after data fetching
- [ ] How to optimize "concept synergy" query for concept v? (Naive: `O(n * pred(x,v))`)
  - [ ] Caching: Precalculate and store
  - [ ] Online Algorithm: Enter new concept (Get embeddings, likely to work better if word embeddings are used in addition)

# My handy tools

## Abstract checking

Retrieve abstract for work given work ID: `$ abstract W2159161622`

## Identifying Sourcce

Retrieve source for work given work ID: `$ getsource W2159161622`
